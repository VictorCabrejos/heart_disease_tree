{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import operator\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into a dataframe\n",
    "df = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "5   57    1   0       140   192    0        1      148      0      0.4      1   \n",
       "6   56    0   1       140   294    0        0      153      0      1.3      1   \n",
       "7   44    1   1       120   263    0        1      173      0      0.0      2   \n",
       "8   52    1   2       172   199    1        1      162      0      0.5      2   \n",
       "9   57    1   2       150   168    0        1      174      0      1.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  \n",
       "5   0     1       1  \n",
       "6   0     2       1  \n",
       "7   0     3       1  \n",
       "8   0     3       1  \n",
       "9   0     2       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.target.unique()\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe dimensions\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are: \n",
    "> 1. age\n",
    "> 2. sex\n",
    "> 3. chest pain type (4 values)\n",
    "> 4. resting blood pressure\n",
    "> 5. serum cholestoral in mg/dl\n",
    "> 6. fasting blood sugar > 120 mg/dl\n",
    "> 7. resting electrocardiographic results (values 0,1,2)\n",
    "> 8. maximum heart rate achieved\n",
    "> 9. exercise induced angina\n",
    "> 10. oldpeak = ST depression induced by exercise relative to rest\n",
    "> 11. the slope of the peak exercise ST segment\n",
    "> 12. number of major vessels (0-3) colored by flourosopy\n",
    "> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "Target is:\n",
    "\n",
    "> The \"goal\" field refers to the presence of heart disease in the patient. \n",
    "\n",
    "> 0 (no presence) , 1 (heart disease) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing / Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14) (212, 14) (30, 14) (61, 14)\n"
     ]
    }
   ],
   "source": [
    "# Creating features and targets\n",
    "\n",
    "features_name = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal','target']\n",
    "                 \n",
    "features = df[features_name]\n",
    "\n",
    "# targets = df['target']\n",
    "\n",
    "#Please split the data into training set (70%) , validation set (10%), test set (20%)\n",
    "\n",
    "train_size = int(0.70 * features.shape[0])\n",
    "validation_size = int(0.10 * features.shape[0])\n",
    "\n",
    "\n",
    "# Training set (70% of all data rows) --> 212\n",
    "train_features = features[:train_size]\n",
    "# train_targets = targets[:train_size]\n",
    "\n",
    "# Validation set (10% of all data rows) --> 30\n",
    "validation_features = features[train_size : train_size + validation_size]\n",
    "# validation_targets = targets[train_size: train_size + validation_size]\n",
    "\n",
    "# Test set (20% of all data rows) --> 61\n",
    "test_features = features[train_size + validation_size : ]\n",
    "# test_targets = targets[train_size + validation_size: ]\n",
    "\n",
    "print(features.shape, train_features.shape, validation_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 14)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features.to_csv(\"training_set.csv\", index=False)\n",
    "df.to_csv(\"dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/equation1.png\">\n",
    "<img src=\"img/equation2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log2 as log\n",
    "\n",
    "# Machine epsilon\n",
    "# The smallest representable positive number such that: 1.0 + eps != 1.0.\n",
    "#  We used Machine Epsilon to avoid: log(0) or 0 in the denominator\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy node =  0.994264609261905\n"
     ]
    }
   ],
   "source": [
    "# We need to find the ENTROPY and the\n",
    "# INFORMATION GAIN for splitting the data set\n",
    "\n",
    "# Calculating Entropy of column \"target\"\n",
    "\n",
    "entropy_node = 0  #Initialize Entropy\n",
    "\n",
    "# find unique valyes of \"target\"\n",
    "values = df.target.unique()  #Unique objects - '0', '1'\n",
    "\n",
    "# this calculates the entropy:\n",
    "\n",
    "for value in values:\n",
    "    # fraction is 'pm' --> proportion of # elements in a \n",
    "    #                    split group\n",
    "    # example: how many '0's in total number of elements\n",
    "    fraction = df.target.value_counts()[value]/len(df.target) \n",
    "    \n",
    "    # summation\n",
    "    entropy_node += - fraction * np.log2(fraction)\n",
    "\n",
    "print(\"Entropy node = \", entropy_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([233, 250, 204, 236, 354, 192, 294, 263, 199, 168, 239, 275, 266,\n",
       "       211, 283, 219, 340, 226, 247, 234, 243, 302, 212, 175, 417, 197,\n",
       "       198, 177, 273, 213, 304, 232, 269, 360, 308, 245, 208, 264, 321,\n",
       "       325, 235, 257, 216, 256, 231, 141, 252, 201, 222, 260, 182, 303,\n",
       "       265, 309, 186, 203, 183, 220, 209, 258, 227, 261, 221, 205, 240,\n",
       "       318, 298, 564, 277, 214, 248, 255, 207, 223, 288, 160, 394, 315,\n",
       "       246, 244, 270, 195, 196, 254, 126, 313, 262, 215, 193, 271, 268,\n",
       "       267, 210, 295, 306, 178, 242, 180, 228, 149, 278, 253, 342, 157,\n",
       "       286, 229, 284, 224, 206, 167, 230, 335, 276, 353, 225, 330, 290,\n",
       "       172, 305, 188, 282, 185, 326, 274, 164, 307, 249, 341, 407, 217,\n",
       "       174, 281, 289, 322, 299, 300, 293, 184, 409, 259, 200, 327, 237,\n",
       "       218, 319, 166, 311, 169, 187, 176, 241, 131], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chol.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43452808237653207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute = 'chol'\n",
    "target_variables = df.target.unique()  #This gives all '0' and '1'\n",
    "variables = df[attribute].unique()    #This gives different features in that attribute (like '233')\n",
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute]==variable][df.target ==target_variable]) #numerator\n",
    "        den = len(df[attribute][df[attribute]==variable])  #denominator\n",
    "        fraction = num/(den+eps)  #pi\n",
    "        entropy_each_feature += -fraction*log(fraction+eps) #This calculates entropy for one feature like \n",
    "    fraction2 = den/len(df)\n",
    "    entropy_attribute += -fraction2*entropy_each_feature   #Sums up all the entropy 'chol'\n",
    "    \n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy(df):\n",
    "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "    entropy = 0\n",
    "    values = df[Class].unique()\n",
    "    for value in values:\n",
    "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy\n",
    "  \n",
    "  \n",
    "def find_entropy_attribute(df,attribute):\n",
    "  Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "  target_variables = df[Class].unique()  #\n",
    "  variables = df[attribute].unique()    #\n",
    "  entropy2 = 0\n",
    "  for variable in variables:\n",
    "      entropy = 0\n",
    "      for target_variable in target_variables:\n",
    "          num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "          den = len(df[attribute][df[attribute]==variable])\n",
    "          fraction = num/(den+eps)\n",
    "          entropy += -fraction*log(fraction+eps)\n",
    "      fraction2 = den/len(df)\n",
    "      entropy2 += -fraction2*entropy\n",
    "  return abs(entropy2)\n",
    "\n",
    "\n",
    "def find_winner(df):\n",
    "    Entropy_att = []\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:\n",
    "#         Entropy_att.append(find_entropy_attribute(df,key))\n",
    "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]\n",
    "  \n",
    "  \n",
    "def get_subtable(df, node,value):\n",
    "  return df[df[node] == value].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def buildTree(df,tree=None): \n",
    "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "    \n",
    "    #Here we build our decision tree\n",
    "\n",
    "    #Get attribute with maximum information gain\n",
    "    node = find_winner(df)\n",
    "    \n",
    "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
    "    attValue = np.unique(df[node])\n",
    "    \n",
    "    #Create an empty dictionary to create tree    \n",
    "    if tree is None:                    \n",
    "        tree={}\n",
    "        tree[node] = {}\n",
    "    \n",
    "   #We make loop to construct a tree by calling this function recursively. \n",
    "    #In this we check if the subset is pure and stops if it is pure. \n",
    "\n",
    "    for value in attValue:\n",
    "        \n",
    "        subtable = get_subtable(df,node,value)\n",
    "        clValue,counts = np.unique(subtable['target'],return_counts=True)                        \n",
    "        \n",
    "        if len(counts)==1:#Checking purity of subset\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable) #Calling the function recursively \n",
    "                   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildTree(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chol': {126: 1,\n",
      "          131: 0,\n",
      "          141: 1,\n",
      "          149: {'age': {49: 0, 71: 1}},\n",
      "          157: 1,\n",
      "          160: 1,\n",
      "          164: 0,\n",
      "          166: 0,\n",
      "          167: 0,\n",
      "          168: 1,\n",
      "          169: 0,\n",
      "          172: 0,\n",
      "          174: 0,\n",
      "          175: 1,\n",
      "          176: 0,\n",
      "          177: {'age': {43: 0, 46: 1, 59: 0, 65: 1}},\n",
      "          178: 1,\n",
      "          180: 1,\n",
      "          182: 1,\n",
      "          183: 1,\n",
      "          184: 0,\n",
      "          185: 0,\n",
      "          186: 1,\n",
      "          187: 0,\n",
      "          188: 0,\n",
      "          192: 1,\n",
      "          193: {'age': {56: 1, 68: 0}},\n",
      "          195: 1,\n",
      "          196: 1,\n",
      "          197: {'age': {44: 0, 46: 1, 53: 1, 58: 1, 63: 0, 76: 1}},\n",
      "          198: {'age': {35: 0, 41: 1}},\n",
      "          199: 1,\n",
      "          200: 0,\n",
      "          201: 1,\n",
      "          203: {'age': {41: 1, 53: 0, 61: 0}},\n",
      "          204: {'age': {29: 1, 41: 1, 46: 1, 47: 1, 52: 0, 59: 0}},\n",
      "          205: {'age': {52: 1, 55: 0}},\n",
      "          206: 0,\n",
      "          207: {'age': {57: 1, 61: 0}},\n",
      "          208: 1,\n",
      "          209: 1,\n",
      "          210: 1,\n",
      "          211: 1,\n",
      "          212: {'age': {52: 0, 59: 1, 64: 0, 66: 0, 67: 0}},\n",
      "          213: 1,\n",
      "          214: 1,\n",
      "          215: 1,\n",
      "          216: {'age': {53: 1, 58: 0}},\n",
      "          217: 0,\n",
      "          218: 0,\n",
      "          219: {'age': {39: 0, 44: 1, 50: 1}},\n",
      "          220: 1,\n",
      "          221: 1,\n",
      "          222: 1,\n",
      "          223: {'age': {40: 0, 52: 1, 67: 1}},\n",
      "          224: 0,\n",
      "          225: 0,\n",
      "          226: 1,\n",
      "          227: 1,\n",
      "          228: {'sex': {0: 0, 1: 1}},\n",
      "          229: 0,\n",
      "          230: 0,\n",
      "          231: {'age': {38: 0, 46: 0, 62: 1}},\n",
      "          232: {'age': {54: 1, 57: 0}},\n",
      "          233: {'age': {44: 1, 50: 0, 52: 1, 63: 1}},\n",
      "          234: {'age': {45: 1, 53: 1, 58: 0, 59: 1, 61: 0, 69: 1}},\n",
      "          235: 1,\n",
      "          236: {'age': {45: 1, 56: 1, 57: 0}},\n",
      "          237: 0,\n",
      "          239: {'thalach': {126: 0, 142: 0, 151: 1, 160: 1}},\n",
      "          240: 1,\n",
      "          241: 0,\n",
      "          242: 1,\n",
      "          243: {'age': {46: 1, 47: 0, 50: 0, 61: 1}},\n",
      "          244: {'age': {42: 1, 50: 1, 62: 0}},\n",
      "          245: 1,\n",
      "          246: {'age': {53: 1, 64: 0, 66: 0}},\n",
      "          247: {'trestbps': {132: 0, 150: 1}},\n",
      "          248: {'age': {58: 1, 65: 0}},\n",
      "          249: 0,\n",
      "          250: 1,\n",
      "          252: 1,\n",
      "          253: {'age': {47: 1, 60: 0}},\n",
      "          254: {'age': {50: 1, 63: 0, 65: 0, 67: 0, 69: 0}},\n",
      "          255: {'age': {48: 1, 52: 0}},\n",
      "          256: {'age': {48: 0, 51: 1, 56: 0}},\n",
      "          257: 1,\n",
      "          258: {'trestbps': {120: 1, 125: 0, 150: 0}},\n",
      "          259: 0,\n",
      "          260: {'age': {45: 1, 61: 0}},\n",
      "          261: {'age': {51: 1, 57: 0}},\n",
      "          262: 1,\n",
      "          263: {'age': {44: 1, 62: 0, 64: 1}},\n",
      "          264: {'age': {45: 0, 53: 1}},\n",
      "          265: 1,\n",
      "          266: {'age': {49: 1, 54: 0}},\n",
      "          267: {'age': {54: 1, 62: 0}},\n",
      "          268: {'age': {41: 1, 62: 0}},\n",
      "          269: {'age': {49: 1, 63: 0, 65: 1, 70: 0, 74: 1}},\n",
      "          270: {'age': {58: 0, 59: 1}},\n",
      "          271: 1,\n",
      "          273: {'age': {54: 1, 59: 0}},\n",
      "          274: 0,\n",
      "          275: {'age': {47: 0, 48: 1}},\n",
      "          276: 0,\n",
      "          277: 1,\n",
      "          278: 1,\n",
      "          281: 0,\n",
      "          282: 0,\n",
      "          283: {'age': {54: 0, 56: 0, 58: 1}},\n",
      "          284: 0,\n",
      "          286: 0,\n",
      "          288: {'age': {54: 1, 56: 0, 59: 0}},\n",
      "          289: 0,\n",
      "          290: 0,\n",
      "          293: 0,\n",
      "          294: {'age': {56: 1, 62: 0}},\n",
      "          295: 1,\n",
      "          298: {'age': {51: 0, 52: 1}},\n",
      "          299: 0,\n",
      "          300: 0,\n",
      "          302: 1,\n",
      "          303: 1,\n",
      "          304: {'age': {54: 1, 77: 0}},\n",
      "          305: 0,\n",
      "          306: 1,\n",
      "          307: 0,\n",
      "          308: 1,\n",
      "          309: {'age': {45: 0, 54: 1, 64: 0}},\n",
      "          311: 0,\n",
      "          313: 1,\n",
      "          315: {'age': {42: 0, 43: 1}},\n",
      "          318: {'age': {58: 0, 60: 1}},\n",
      "          319: 0,\n",
      "          321: 1,\n",
      "          322: 0,\n",
      "          325: 1,\n",
      "          326: 0,\n",
      "          327: 0,\n",
      "          330: 0,\n",
      "          335: 0,\n",
      "          340: 1,\n",
      "          341: 0,\n",
      "          342: 1,\n",
      "          353: 0,\n",
      "          354: 1,\n",
      "          360: 1,\n",
      "          394: 1,\n",
      "          407: 0,\n",
      "          409: 0,\n",
      "          417: 1,\n",
      "          564: 1}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inst,tree):\n",
    "    #This function is used to predict for any input variable \n",
    "    \n",
    "    #Recursively we go through the tree that we built earlier\n",
    "\n",
    "    for nodes in tree.keys():        \n",
    "        \n",
    "        value = inst[nodes]\n",
    "        tree = tree[nodes][value]\n",
    "        prediction = 0\n",
    "            \n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(inst, tree)\n",
    "        else:\n",
    "            prediction = tree\n",
    "            break;                            \n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = df.iloc[6]  #This takes row with index 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          56.0\n",
       "sex           0.0\n",
       "cp            1.0\n",
       "trestbps    140.0\n",
       "chol        294.0\n",
       "fbs           0.0\n",
       "restecg       0.0\n",
       "thalach     153.0\n",
       "exang         0.0\n",
       "oldpeak       1.3\n",
       "slope         1.0\n",
       "ca            0.0\n",
       "thal          2.0\n",
       "target        1.0\n",
       "Name: 6, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'age':56.0, 'sex':0.0, 'cp':1.0, 'trestbps':140.0, 'chol':294.0, 'fbs':0.0, 'restecg':0.0, 'thalach':153.0, 'exang':0.0, 'oldpeak':1.3, 'slope':1.0, 'ca':0.0, 'thal':2.0}\n",
    "inst = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(inst, tree)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvdata class to store the csv data\n",
    "class csvdata():\n",
    "    def __init__(self, classifier):\n",
    "        self.rows = []\n",
    "        self.attributes = []\n",
    "        self.attribute_types = []\n",
    "        self.classifier = classifier\n",
    "        self.class_col_index = None\n",
    "\n",
    "# the node class that will make up the tree\n",
    "class decisionTreeNode():\n",
    "    def __init__(self, is_leaf_node, classification, attribute_split_index, attribute_split_value, parent, left_child, right_child, height):\n",
    "\n",
    "        self.classification = None\n",
    "        self.attribute_split = None\n",
    "        self.attribute_split_index = None\n",
    "        self.attribute_split_value = None\n",
    "        self.parent = parent\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.height = None\n",
    "        self.is_leaf_node = True\n",
    "\n",
    "def preprocessing(dataset):\n",
    "\n",
    "    #convert attributes that are numeric to floats. \n",
    "    for example in dataset.rows:\n",
    "        for x in range(len(dataset.rows[0])):\n",
    "            if dataset.attributes[x] == 'True':\n",
    "                example[x] = float(example[x])\n",
    "\n",
    "\n",
    "# compute the decision tree recursively\n",
    "\n",
    "def compute_decision_tree(dataset, parent_node, classifier, cut_off_value):\n",
    "    \n",
    "    node = decisionTreeNode(True, None, None, None, parent_node, None, None, 0)\n",
    "    if (parent_node == None):\n",
    "        node.height = 0\n",
    "    else:\n",
    "        node.height = node.parent.height + 1\n",
    "\n",
    "    #count_positives() will count the number of rows with classification \"1\"\n",
    "    ones = count_positives(dataset.rows, dataset.attributes, classifier)\n",
    "\n",
    "    if (len(dataset.rows) == ones):\n",
    "        node.classification = 1\n",
    "        node.is_leaf_node = True\n",
    "        return node\n",
    "    elif (ones == 0):\n",
    "        node.is_leaf_node = True\n",
    "        node.classification = 0\n",
    "        return node\n",
    "    else:\n",
    "        node.is_leaf_node = False\n",
    "\n",
    "    # The index of the attribute we will split on\n",
    "    splitting_attribute = None\n",
    "\n",
    "    # The information gain given by the best attribute\n",
    "    maximum_info_gain = 0\n",
    "\n",
    "    split_val = None\n",
    "\n",
    "    # CUT-OFF VALUES : \n",
    "    # [0.2, 0.4, 0.6, 0.8]\n",
    "    minimum_info_gain = cut_off_value/10\n",
    "\n",
    "    entropy = calculate_entropy(dataset, classifier)\n",
    "\n",
    "    #for each column of data\n",
    "    for attr_index in range(len(dataset.rows[0])):\n",
    "\n",
    "        if (dataset.attributes[attr_index] != classifier):\n",
    "            local_max_gain = 0\n",
    "            local_split_val = None\n",
    "            attr_value_list = [example[attr_index] for example in dataset.rows] # these are the values we can split on, now we must find the best one\n",
    "            attr_value_list = list(set(attr_value_list)) # remove duplicates from list of all attribute values\n",
    "\n",
    "            if(len(attr_value_list) > 100):\n",
    "                attr_value_list = sorted(attr_value_list)\n",
    "                total = len(attr_value_list)\n",
    "                ten_percentile = int(total/10)\n",
    "                new_list = []\n",
    "                for x in range(1, 10):\n",
    "                    new_list.append(attr_value_list[x*ten_percentile])\n",
    "                attr_value_list = new_list\n",
    "\n",
    "            for val in attr_value_list:\n",
    "                # calculate the gain if we split on this value\n",
    "                # if gain is greater than local_max_gain, save this gain and this value\n",
    "                current_gain = calculate_information_gain(attr_index, dataset,val,entropy)\n",
    "\n",
    "                if (current_gain > local_max_gain):\n",
    "                    local_max_gain = current_gain\n",
    "                    local_split_val = val\n",
    "\n",
    "            if (local_max_gain > maximum_info_gain):\n",
    "                maximum_info_gain = local_max_gain\n",
    "                split_val = local_split_val\n",
    "                splitting_attribute = attr_index\n",
    "                \n",
    "    \n",
    "    # CUT-OFF VALUES : \n",
    "    # [0.2, 0.4, 0.6, 0.8]\n",
    "    if (maximum_info_gain <= minimum_info_gain or node.height > 20):\n",
    "        \n",
    "        node.is_leaf_node = True\n",
    "        node.classification = classify_leaf(dataset, classifier)\n",
    "        return node\n",
    "\n",
    "    node.attribute_split_index = splitting_attribute\n",
    "    node.attribute_split = dataset.attributes[splitting_attribute]\n",
    "    node.attribute_split_value = split_val\n",
    "\n",
    "    left_dataset = csvdata(classifier)\n",
    "    right_dataset = csvdata(classifier)\n",
    "\n",
    "    left_dataset.attributes = dataset.attributes\n",
    "    right_dataset.attributes = dataset.attributes\n",
    "\n",
    "    left_dataset.attribute_types = dataset.attribute_types\n",
    "    right_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for row in dataset.rows:\n",
    "        if (splitting_attribute is not None and row[splitting_attribute] >= split_val):\n",
    "            left_dataset.rows.append(row)\n",
    "        elif (splitting_attribute is not None):\n",
    "            right_dataset.rows.append(row)\n",
    "\n",
    "    node.left_child = compute_decision_tree(left_dataset, node, classifier, cut_off_value)\n",
    "    node.right_child = compute_decision_tree(right_dataset, node, classifier, cut_off_value)\n",
    "\n",
    "    return node\n",
    "\n",
    "# Classify dataset\n",
    "def classify_leaf(dataset, classifier):\n",
    "    ones = count_positives(dataset.rows, dataset.attributes, classifier)\n",
    "    total = len(dataset.rows)\n",
    "    zeroes = total - ones\n",
    "    if (ones >= zeroes):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Final evaluation of the data\n",
    "def get_classification(example, node, class_col_index):\n",
    "    if (node.is_leaf_node == True):\n",
    "        return node.classification\n",
    "    else:\n",
    "        if (example[node.attribute_split_index] >= node.attribute_split_value):\n",
    "            return get_classification(example, node.left_child, class_col_index)\n",
    "        else:\n",
    "            return get_classification(example, node.right_child, class_col_index)\n",
    "\n",
    "##################################################\n",
    "# Calculate the entropy of the current dataset\n",
    "##################################################\n",
    "def calculate_entropy(dataset, classifier):\n",
    "\n",
    "    #get count of all the rows with classification 1\n",
    "    ones = count_positives(dataset.rows, dataset.attributes, classifier)\n",
    "\n",
    "    #get the count of all the rows in the dataset.\n",
    "    total_rows = len(dataset.rows);\n",
    "    #from the above two we can get the count of rows with classification 0 too\n",
    "\n",
    "    #Entropy formula is sum of p*log2(p). Referred the slides. P is the probability\n",
    "    entropy = 0\n",
    "\n",
    "    #probability p of classification 1 in total data\n",
    "    p = ones / total_rows\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "    #probability p of classification 0 in total data\n",
    "    p = (total_rows - ones)/total_rows\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "\n",
    "    #from the formula\n",
    "    entropy = -entropy\n",
    "    return entropy\n",
    "\n",
    "##################################################\n",
    "# Calculate the gain of a particular attribute split\n",
    "##################################################\n",
    "def calculate_information_gain(attr_index, dataset, val, entropy):\n",
    "\n",
    "    classifier = dataset.attributes[attr_index]\n",
    "    attr_entropy = 0\n",
    "    total_rows = len(dataset.rows);\n",
    "    gain_upper_dataset = csvdata(classifier)\n",
    "    gain_lower_dataset = csvdata(classifier)\n",
    "    gain_upper_dataset.attributes = dataset.attributes\n",
    "    gain_lower_dataset.attributes = dataset.attributes\n",
    "    gain_upper_dataset.attribute_types = dataset.attribute_types\n",
    "    gain_lower_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for example in dataset.rows:\n",
    "        if (example[attr_index] >= val):\n",
    "            gain_upper_dataset.rows.append(example)\n",
    "        elif (example[attr_index] < val):\n",
    "            gain_lower_dataset.rows.append(example)\n",
    "\n",
    "    if (len(gain_upper_dataset.rows) == 0 or len(gain_lower_dataset.rows) == 0):\n",
    "        return -1\n",
    "\n",
    "    attr_entropy += calculate_entropy(gain_upper_dataset, classifier) * len(gain_upper_dataset.rows) / total_rows\n",
    "    attr_entropy += calculate_entropy(gain_lower_dataset, classifier) * len(gain_lower_dataset.rows) / total_rows\n",
    "\n",
    "    return entropy - attr_entropy\n",
    "\n",
    "##################################################\n",
    "# count number of rows with classification \"1\"\n",
    "##################################################\n",
    "def count_positives(instances, attributes, classifier):\n",
    "    count = 0\n",
    "    class_col_index = None\n",
    "\n",
    "    #find the index of classifier\n",
    "    for a in range(len(attributes)):\n",
    "        if attributes[a] == classifier:\n",
    "            class_col_index = a\n",
    "        else:\n",
    "            class_col_index = len(attributes) - 1\n",
    "    for i in instances:\n",
    "        if i[class_col_index] == \"1\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def validate_tree(node, dataset):\n",
    "    total = len(dataset.rows)\n",
    "    correct = 0\n",
    "    for row in dataset.rows:\n",
    "        # validate example\n",
    "        correct += validate_row(node, row)\n",
    "    return correct/total\n",
    "\n",
    "# Validate row (for finding best score before pruning)\n",
    "def validate_row(node, row):\n",
    "    if (node.is_leaf_node == True):\n",
    "        projected = node.classification\n",
    "        actual = int(row[-1])\n",
    "        if (projected == actual):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    value = row[node.attribute_split_index]\n",
    "    if (value >= node.attribute_split_value):\n",
    "        return validate_row(node.left_child, row)\n",
    "    else:\n",
    "        return validate_row(node.right_child, row)\n",
    "\n",
    "##################################################\n",
    "# Prune tree\n",
    "##################################################\n",
    "def prune_tree(root, node, validate_set, best_score):\n",
    "    # if node is a leaf\n",
    "    if (node.is_leaf_node == True):\n",
    "        classification = node.classification\n",
    "        node.parent.is_leaf_node = True\n",
    "        node.parent.classification = node.classification\n",
    "        if (node.height < 20):\n",
    "            new_score = validate_tree(root, validate_set)\n",
    "        else:\n",
    "            new_score = 0\n",
    "\n",
    "        if (new_score >= best_score):\n",
    "            return new_score\n",
    "        else:\n",
    "            node.parent.is_leaf_node = False\n",
    "            node.parent.classification = None\n",
    "            return best_score\n",
    "    # if its not a leaf\n",
    "    else:\n",
    "        new_score = prune_tree(root, node.left_child, validate_set, best_score)\n",
    "        if (node.is_leaf_node == True):\n",
    "            return new_score\n",
    "        new_score = prune_tree(root, node.right_child, validate_set, new_score)\n",
    "        if (node.is_leaf_node == True):\n",
    "            return new_score\n",
    "\n",
    "        return new_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree(cut_off_value):\n",
    "\n",
    "    dataset = csvdata(\"\")\n",
    "    training_set = csvdata(\"\")\n",
    "    test_set = csvdata(\"\")\n",
    "\n",
    "    # Load data set\n",
    "    #with open(\"data.csv\") as f:\n",
    "    #    dataset.rows = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    #print \"Number of records: %d\" % len(dataset.rows)\n",
    "    \n",
    "    f = open(\"dataset.csv\")\n",
    "    original_file = f.read()\n",
    "    rowsplit_data = original_file.splitlines()\n",
    "    dataset.rows = [rows.split(',') for rows in rowsplit_data]\n",
    "\n",
    "\n",
    "    dataset.attributes = dataset.rows.pop(0)\n",
    "    print(dataset.attributes)\n",
    "\n",
    "    # this is used to generalize the code for other datasets.\n",
    "    # true indicates numeric data. false in nominal data\n",
    "    dataset.attribute_types = ['true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'false']\n",
    "\n",
    "\n",
    "    classifier = dataset.attributes[-1]\n",
    "    dataset.classifier = classifier\n",
    "\n",
    "\n",
    "    # find index of classifier\n",
    "    for a in range(len(dataset.attributes)):\n",
    "        if dataset.attributes[a] == dataset.classifier:\n",
    "            dataset.class_col_index = a\n",
    "        else:\n",
    "            dataset.class_col_index = range(len(dataset.attributes))[-1]\n",
    "\n",
    "    print(\"classifier is %d\" % dataset.class_col_index)\n",
    "    \n",
    "    # preprocessing the dataset\n",
    "    preprocessing(dataset)\n",
    "\n",
    "    training_set = copy.deepcopy(dataset)\n",
    "    training_set.rows = []\n",
    "    \n",
    "    test_set = copy.deepcopy(dataset)\n",
    "    test_set.rows = []\n",
    "    \n",
    "    validate_set = copy.deepcopy(dataset)\n",
    "    validate_set.rows = []\n",
    "    \n",
    "    \n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "\n",
    "    ##This is to create a validation set for post pruning\n",
    "    dataset.rows = [x for i, x in enumerate(dataset.rows) if i % 10 != 9]\n",
    "    validate_set.rows = [x for i, x in enumerate(dataset.rows) if i % 10 == 9]\n",
    "\n",
    "    K=10\n",
    "    # Stores accuracy of the 10 runs\n",
    "    accuracy = []\n",
    "    start =  time.process_time()\n",
    "    for k in range(K):\n",
    "        print(\"Doing fold \", k)\n",
    "        training_set.rows = [x for i, x in enumerate(dataset.rows) if i % K != k]\n",
    "        test_set.rows = [x for i, x in enumerate(dataset.rows) if i % K == k]\n",
    "\n",
    "        print(\"Number of training records: %d\" % len(training_set.rows))\n",
    "        print(\"Number of test records: %d\" % len(test_set.rows))\n",
    "        root = compute_decision_tree(training_set, None, classifier, cut_off_value)\n",
    "\n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        \n",
    "        for instance in test_set.rows:\n",
    "            result = get_classification(instance, root, test_set.class_col_index)\n",
    "            results.append(str(result) == str(instance[-1]))\n",
    "\n",
    "        # Accuracy\n",
    "        acc = float(results.count(True))/float(len(results))\n",
    "        print(\"accuracy: %.4f\" % acc)\n",
    "\n",
    "        # pruning code:\n",
    "        best_score = validate_tree(root, validate_set)\n",
    "        post_prune_accuracy = 100*prune_tree(root, root, validate_set, best_score)\n",
    "        print(\"Post-pruning score on validation set: \" + str(post_prune_accuracy) + \"%\")\n",
    "        \n",
    "        accuracy.append(acc)\n",
    "        del root\n",
    "\n",
    "    mean_accuracy = math.fsum(accuracy)/10\n",
    "    print(\"Accuracy  %f \" % (mean_accuracy))\n",
    "    print(\"Took %f secs\" % ( time.process_time() - start))\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % mean_accuracy)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "classifier is 13\n",
      "Doing fold  0\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  1\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  2\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8571\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  3\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7778\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  4\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  5\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  6\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  7\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  8\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  9\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7778\n",
      "Post-pruning score on validation set: 81.48148148148148%\n",
      "Accuracy  0.764815 \n",
      "Took 0.765625 secs\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.2\n",
    "run_decision_tree(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "classifier is 13\n",
      "Doing fold  0\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  1\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  2\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8571\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  3\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7778\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  4\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  5\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  6\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  7\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  8\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  9\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7778\n",
      "Post-pruning score on validation set: 81.48148148148148%\n",
      "Accuracy  0.764815 \n",
      "Took 0.765625 secs\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.4\n",
    "run_decision_tree(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "classifier is 13\n",
      "Doing fold  0\n",
      "Number of training records: 171\n",
      "Number of test records: 20\n",
      "accuracy: 0.8500\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  1\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8947\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  2\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 1.0000\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  3\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.7368\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  4\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8421\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  5\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8421\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  6\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.9474\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  7\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8421\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  8\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8421\n",
      "Post-pruning score on validation set: 94.73684210526315%\n",
      "Doing fold  9\n",
      "Number of training records: 172\n",
      "Number of test records: 19\n",
      "accuracy: 0.8947\n",
      "Post-pruning score on validation set: 89.47368421052632%\n",
      "Accuracy  0.869211 \n",
      "Took 0.343750 secs\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.6\n",
    "run_decision_tree(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
      "classifier is 13\n",
      "Doing fold  0\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  1\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8214\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  2\n",
      "Number of training records: 245\n",
      "Number of test records: 28\n",
      "accuracy: 0.8571\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  3\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.8148\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  4\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  5\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7407\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  6\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  7\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  8\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7037\n",
      "Post-pruning score on validation set: 100.0%\n",
      "Doing fold  9\n",
      "Number of training records: 246\n",
      "Number of test records: 27\n",
      "accuracy: 0.7778\n",
      "Post-pruning score on validation set: 81.48148148148148%\n",
      "Accuracy  0.768519 \n",
      "Took 0.734375 secs\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.8\n",
    "run_decision_tree(cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "complexity = np.arange(0.1, 2, 0.1)\n",
    "train_error = -np.log(complexity)\n",
    "test_error = -np.log(complexity) + np.power(complexity, 1)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.xlabel(\"Algorithm complexity\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "plt.plot(complexity, train_error, 'C0o-', label='Training error')\n",
    "plt.plot(complexity, test_error, 'C1o-', label=\"Test error\")\n",
    "\n",
    "plt.text(0.1, 0.25, \"$\\longleftarrow$ high bias\", color='C0')\n",
    "plt.text(1.5, 0.25, \"high variance $\\longrightarrow$\", color='C1')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
